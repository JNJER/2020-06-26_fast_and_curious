{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this benchmark is to offer a comparison between differents pre-trained image recognition's networks. Here the recognition is on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for 1000 labels. Some image's transformations like grayscale or downsampling will be implemented too to infer on their influence on the accuracy of the networks.    \n",
    "\n",
    "On this notebook I used the [Pytorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This Notebook was done during a master 1 internship by [Jean-Nicolas Jérémie](https://github.com/JNJER) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/), researcher at the Neurosciences Institute of Timone (INT).\n",
    "\n",
    "<!-- TEASER_END -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing librarys and definition of dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%mkdir -p DCNN_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile DCNN_benchmark/init.py\n",
    "# import libs\n",
    "import os\n",
    "import time \n",
    "from time import strftime,gmtime\n",
    "import json\n",
    "import time \n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "from numpy import random\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "# figure's variables\n",
    "fig_width = 20\n",
    "phi = (np.sqrt(5)+1)/2\n",
    "phi = phi**2\n",
    "colors = ['b', 'r', 'k','g']\n",
    "\n",
    "# host & date's variables \n",
    "HOST = os.uname()[1]\n",
    "#datetag = strftime(\"%Y-%m-%d\", gmtime()) \n",
    "datetag = '2020-08-27'\n",
    "\n",
    "#dataset configuration\n",
    "\n",
    "image_size = 256 # default image resolution\n",
    "image_sizes = 2**np.arange(6, 10) # resolutions explored\n",
    "\n",
    "N_images_per_class = 100\n",
    "#i_labels = random.randint(1000, size=(N_labels)) # Random choice\n",
    "i_labels = [409, 530, 892, 487, 920, 704, 879, 963, 646, 620 ] # Pre-selected classes\n",
    "N_labels = len(i_labels)\n",
    "\n",
    "id_dl = ''\n",
    "root = 'data'\n",
    "folder = 'imagenet_classes_100'\n",
    "path = os.path.join(root, folder) # data path\n",
    "\n",
    "with open('ImageNet-datasets-downloader/imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "labels[0].split(', ')\n",
    "labels = [label.split(', ')[1].lower().replace('_', ' ') for label in labels]\n",
    "\n",
    "class_loader = 'ImageNet-datasets-downloader/imagenet_class_info.json'\n",
    "with open(class_loader, 'r') as fp: # get all the classes on the data_downloader\n",
    "    name = json.load(fp)\n",
    "\n",
    "# a reverse look-up-table giving the index of a given label (within the whole set of imagenet labels)\n",
    "reverse_labels = {}\n",
    "for i_label, label in enumerate(labels):\n",
    "    reverse_labels[label] = i_label\n",
    "# a reverse look-up-table giving the index of a given i_label (within the sub-set of classes)\n",
    "reverse_i_labels = {}\n",
    "for i_label, label in enumerate(i_labels):\n",
    "    reverse_i_labels[label] = i_label\n",
    "\n",
    "print('-'*24)\n",
    "# choosing the selected classes for recognition\n",
    "for i_label in i_labels: \n",
    "    print('label', i_label, '=', labels[i_label])\n",
    "    for key in name:\n",
    "        if name[key]['class_name'] == labels[i_label]:\n",
    "            id_dl += key + ' '\n",
    "print('label IDs = ', id_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DCNN_benchmark/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOST == 'fortytwo':\n",
    "    do_local = False \n",
    "    python_exec = \"KMP_DUPLICATE_LIB_OK=TRUE python3\"\n",
    "else :\n",
    "    do_local =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download of example images from ImageNet :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an [ImageNet dataloader](https://github.com/laurentperrinet/ImageNet-datasets-downloader) to populate a dataset based on the pre-selected or randoms classes listed in the `DCNN_benchmark/init.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_benchmark/dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_benchmark.init import *\n",
    "\n",
    "list_dir = os.listdir(path)\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    print(\"The folder \" , folder, \" already exists, it includes: \", list_dir)   # check if the folder exist and create one \n",
    "    \n",
    "else :\n",
    "    print(f\"No existing path match for this folder, creating a folder at {path}\")\n",
    "    os.makedirs(path)\n",
    "\n",
    "print(list_dir)\n",
    "if len(list_dir) < N_labels : # if there aren't anough labels download some more\n",
    "    print('This folder do not have anough classes, downloading some more') # using the downloader\n",
    "    cmd =f\"python3 ImageNet-datasets-downloader/downloader.py -data_root {root} -data_folder {folder} -images_per_class {N_images_per_class} -use_class_list True  -class_list {id_dl} -multiprocessing_workers 0\"\n",
    "    print('Command to run : ', cmd)\n",
    "    os.system(cmd) # running it\n",
    "    list_dir = os.listdir(path)\n",
    "    \n",
    "elif len(os.listdir(path)) == N_labels :\n",
    "    print(f'The folder already contains : {len(list_dir)} classes')\n",
    "          \n",
    "else : # if there are to many folders delete some\n",
    "    print('The folder have to many classes, deleting some')\n",
    "    while len(os.listdir(path)) > N_labels : \n",
    "        for elem in os.listdir(path):\n",
    "            contenu = os.listdir(f'{path}/{elem}')\n",
    "            for x in contenu:\n",
    "                os.remove(f'{path}/{elem}/{x}') # delete exces folders\n",
    "        try:\n",
    "            os.rmdir(f'{path}/{elem}')\n",
    "        except:\n",
    "            os.remove(f'{path}/{elem}')\n",
    "    list_dir = os.listdir(path)\n",
    "    print(\"Now the folder \" , folder, f\" contains :\", os.listdir(path)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !python3 {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained network's import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we worked on four differents pre-trained networks `Alexnet`, `Mobilenet`, `Resnet101` and `VGG16`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_benchmark/models.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_benchmark.init import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transform function for input's image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(image_size)),      # Resize the image to image_size x image_size pixels size.\n",
    "    transforms.CenterCrop(int(image_size-20)),  # Crop the image to (image_size-20) x (image_size-20) pixels around the center.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting its average and\n",
    "                                 #     its standard deviation at the specified values.\n",
    "    mean=[0.485, 0.456, 0.406],                \n",
    "    std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "\n",
    "\n",
    "image_dataset = ImageFolder(path, transform=transform) # save the dataset\n",
    "\n",
    "# imports networks with weights\n",
    "models = {} # get model's names\n",
    "\n",
    "models['alex'] = torchvision.models.alexnet(pretrained=True)\n",
    "models['vgg'] = torchvision.models.vgg16(pretrained=True)\n",
    "models['mob'] = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "models['res'] = torchvision.models.resnext101_32x8d(pretrained=True)\n",
    "\n",
    "\n",
    "# Select a device (CPU or CUDA)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name in models.keys():\n",
    "    models[name].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Image processing and recognition for differents labels :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the classification confidence of the models according to the classes on which they have been trained, i.e. the 1000 classes of the ImageNet library, the softmax mathematical function is added at the output of the networks. The softmax function is a function which takes a vector (here tensor) of dimension K (here K=1000 trained classes) real values and returns for each of these values a normalized propability between 0 and 1 with a sum equal to 1. Thus all the classes are represented in the final vector and a low probability would then be a proof of absence. Nevertheless, the recognition being carried out on so-called \"natural\" images of the irrelevant classes could noise the recognition of those of interest. Thus to reduce this effect, we have applied a slight modification to the output softmax function, it does not recover a vector of K = 1000 ais of K = N_labels. Thus the probabilities obtained would correspond to a confidence of classification discriminating only the classes of interest. \n",
    "\n",
    "We extract differents factors like the accuracy and the processing time for differents datasets at differents resolution in a pandas object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_basic.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(filename)\n",
    "except:\n",
    "    df = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "    \n",
    "    # image preprocessing\n",
    "    for i_image, (data, label) in enumerate(image_dataset):\n",
    "        for name in models.keys():\n",
    "            model = models[name]\n",
    "            model.eval()\n",
    "            tic = time.time()\n",
    "            out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "            percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "            _, indices = torch.sort(percentage, descending=True)           \n",
    "            dt = time.time() - tic\n",
    "            i_label_top = reverse_labels[image_dataset.classes[label]]\n",
    "            perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "            df.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                               'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                               'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'device':str(device)}\n",
    "            print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "            i_trial += 1\n",
    "    df.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents labels display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect our results, we can already display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "df = pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the frequency of the classification performance for our four models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Frequency', size= 18)\n",
    "    axs.set_xlabel('Classification performance (%)', size= 18)\n",
    "    df[df['model']==name]['perf'].plot.hist(bins=np.linspace(0, 100, 20), lw=0, alpha=0.8, label=name)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Process on : ' + str(df['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the 64 worsts classification performance, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df.loc[idx]['label'] + ' | ' + df.loc[idx]['model'], color='g')\n",
    "    perf_ = df.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents compuation time of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    axs.set_ylim(0, 1)\n",
    "    df[df['model']==name]['time'].plot(label=name, color=color, marker='s', lw=0)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Process on : ' + str(df['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it even clearer we extracted a specific median for each models : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"perf\"])\n",
    "    print(f'For the {name} model, the median clasification performance =  {med_perf:.1f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time 's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"time\"])\n",
    "    print(f'For the {name} model, the median computation time  =  {med_perf:.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame per second's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"fps\"])\n",
    "    print(f'For the {name} model, the median fps  =  {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wih seems to present the best accuracy is the `Resnet_101` network, the cost for a high accuracy is represented in the computation time  the network need to compute an image so the Resnet_101 also present the higher computation time. Note that the Mobilenet network show a good accuracy (>80%) while it keeps a reasonable computation's time which come with a high frame per second rate that allows real time recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Image processing and recognition for differents resolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_downsample.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "\n",
    "# Output's set up\n",
    "try:\n",
    "    df_downsample = pd.read_json(filename)\n",
    "except:\n",
    "    df_downsample = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'image_size', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "\n",
    "    # image preprocessing\n",
    "    for image_size in image_sizes:\n",
    "        image_size = int(image_size)\n",
    "        transform = transforms.Compose([  # Downsampling function on the input\n",
    "        transforms.Resize(image_size),      #  Resize the image to image_size x image_size pixels size.\n",
    "        transforms.CenterCrop(image_size),  # Crop the image to image_size x image_size pixels around the center.\n",
    "        transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "        transforms.Normalize(        # Normalize the image by adjusting its average and\n",
    "                                     # its standard deviation at the specified values.\n",
    "        mean=[0.485, 0.456, 0.406],                \n",
    "        std=[0.229, 0.224, 0.225]                  \n",
    "        )])\n",
    "        image_dataset_downsample = ImageFolder(path, transform=transform) # Get the downsample dataset\n",
    "        print(f'Résolution de {image_size}')\n",
    "        # Displays the input image of the model \n",
    "        for i_image, (data, label) in enumerate(image_dataset_downsample):\n",
    "            for name in models.keys():\n",
    "                model = models[name]\n",
    "                model.eval()\n",
    "                tic = time.time()\n",
    "                out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "                percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "                _, indices = torch.sort(percentage, descending=True)           \n",
    "                dt = time.time() - tic\n",
    "                i_label_top = reverse_labels[image_dataset_downsample.classes[label]]\n",
    "                perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "                df_downsample.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'image_size': image_size, 'device':str(device)}\n",
    "                print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "                i_trial += 1\n",
    "        df_downsample.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents resolutions display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, again, we collect our results, and display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "df_downsample = pd.read_json(filename)\n",
    "df_downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the accuracy of each models on the same dataset for differents resolutions :\n",
    "\n",
    "Here accuracies are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"perf\", data=df_downsample, inner=\"quartile\", hue='model',cut = 0)\n",
    "    axs.set_title('Process on : ' + str(df_downsample['device'][0]), size=20)\n",
    "    axs.set_ylabel('Classification performance (%)', size=18)\n",
    "    axs.set_xlabel('Image size', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 64 worsts classification performance, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df_downsample[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df_downsample.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_downsample.loc[idx]['label'] + ' | ' + df_downsample.loc[idx]['model']+ ' | ' + str(df_downsample.loc[idx]['image_size']), color='g')\n",
    "    perf_ = df_downsample.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents compuation time of each models on the same dataset for differents resolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"time\", data=df_downsample, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Process on : ' + str(df_downsample['device'][0]), size = 20)\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    axs.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we extracted a specific median for each models : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_downsample[df_downsample['model']==name][\"perf\"])\n",
    "    print(f'For the {name} model, the median clasification performance =  {med_perf:.3f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification performance does not depend on the host (a priori :-) ) but the timing does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_downsample[df_downsample['model']==name][\"time\"])\n",
    "    print(f'For the {name} model, the median  computation time =  {med_perf:.2f} s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_downsample[df_downsample['model']==name][\"fps\"])\n",
    "    print(f'For the {name} model, the median fps  =  {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance seems reduce when the resolution is to low or to high as the regular size of an input's image is around 256 x 256 pixels. Also, the computation time  seems proportional to the resolution, a higher resolution need a higher delay to compute the image on a CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Image processing and recognition on Grayscale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_grayscale.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_3_{HOST}.json'\n",
    "\n",
    "# Output's set up\n",
    "try:\n",
    "    df_gray = pd.read_json(filename)\n",
    "except:\n",
    "    df_gray = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "    \n",
    "    # image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Grayscale(3),      # convert the image in grayscale\n",
    "    transforms.Resize(int(image_size)),      # Resize the image.\n",
    "    transforms.CenterCrop(int(image_size-20)), # Crop the image with a 20 pixels border.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting its average and\n",
    "                                 #     its standard deviation at the specified values.\n",
    "    mean=[0.485, 0.456, 0.406],                \n",
    "    std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "    image_dataset_grayscale = ImageFolder(path, transform=transform) # Get the downsample dataset\n",
    "\n",
    "    # Displays the input image of the model\n",
    "    for i_image, (data, label) in enumerate(image_dataset_grayscale):\n",
    "            for name in models.keys():\n",
    "                model = models[name]\n",
    "                model.eval()\n",
    "                tic = time.time()\n",
    "                out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "                percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "                _, indices = torch.sort(percentage, descending=True)           \n",
    "                dt = time.time() - tic\n",
    "                i_label_top = reverse_labels[image_dataset_grayscale.classes[label]]\n",
    "                perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "                df_gray.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'device':str(device)}\n",
    "                print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "                i_trial += 1\n",
    "    df_gray.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents labels with grayscale display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all the results, displaying all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_3_{HOST}.json'\n",
    "df_gray = pd.read_json(filename)\n",
    "df_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the accuracy of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Frequency', size= 18)\n",
    "    axs.set_xlabel('Classification performance (%)', size= 18)\n",
    "    df_gray[df_gray['model']==name]['perf'].plot.hist(bins=np.linspace(0, 100, 20), lw=0, alpha=0.8, label=name)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Process on : ' + str(df_gray['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 64 worsts classification performance, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 6\n",
    "N_image_j = 6\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df_gray[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df_gray.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_gray.loc[idx]['label'] + ' | ' + df_gray.loc[idx]['model'], color='g')\n",
    "    perf_ = df_gray.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents computation time of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    df_gray[df_gray['model']==name]['time'].plot(label=name, color=color, marker='s', lw=0)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Process on : ' + str(df_gray['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted a specific median for each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"perf\"])\n",
    "    print(f'For the {name} model, the median clasification performance =  {med_perf:.1f} %' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"time\"])\n",
    "    print(f'For the {name} model, the median computation time  =  {med_perf:.3f} s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"fps\"])\n",
    "    print(f'For the {name} model, the median fps  =  {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grayscale transformation on the input seems to degrade the recognition for all the models as they perform on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the accuracy of each models on the same dataset for differents resolutions :\n",
    "\n",
    "Here accuracies are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, df_, label in zip(['gray', 'red'], [df_gray, df], ['black', 'color']):\n",
    "    axs = sns.violinplot(x=\"model\", y=\"perf\", data=df_, inner=\"quartile\", cut=0, color=color, alpha=.5)\n",
    "    axs.set_title('Process on : ' + str(df_['device'][0]) + ' mode :' + label, size=20)\n",
    "    axs.set_ylabel('Classification performance (%)', size=18)\n",
    "    axs.set_xlabel('Model', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final synthesis\n",
    "\n",
    "We have run the benchmark on various platforms, with or without GPU. Let's summarize the main message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTS = {'fortytwo':'iMac pro 36 cores', \n",
    "         'ai-int-desktop': 'NVIDIA Jetson TX2', \n",
    "        # 'inv-ope-de06': 'Dell station with GTX Tegra',\n",
    "        }\n",
    "for HOST in  HOSTS:\n",
    "    print('HOST:', HOST, ', device:', HOSTS[HOST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the classification performance is similar on different machines (as these algorithms are deterministic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('results/2020-08-27_results_1_fortytwo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOST in HOSTS:\n",
    "    filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "    print(filename)\n",
    "    df = pd.read_json(filename)\n",
    "    for name in models.keys():\n",
    "        med_perf = np.median(df[df['model']==name][\"perf\"])\n",
    "        print(f'On host {HOSTS[HOST]}, for the {name} model, the median clasification performance =  {med_perf:.1f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that the computation time varies greatly depending on the platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOST in HOSTS:\n",
    "    filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "    df = pd.read_json(filename)\n",
    "    for name in models.keys():\n",
    "        med_perf = np.median(df[df['model']==name][\"time\"])\n",
    "        print(f'On host {HOSTS[HOST]}, for the {name} model, the median computation time  =  {med_perf:.3f} s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experiment 2, we check on the different hosts the same trend of classification performance for different image size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_summary = pd.DataFrame([], columns=image_sizes) \n",
    "\n",
    "for name in models.keys():\n",
    "    print(f'For the {name} model' )\n",
    "    for HOST in HOSTS:\n",
    "        filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "        df = pd.read_json(filename)\n",
    "        df_summary.loc[HOSTS[HOST]] = [np.median(df[df['model']==name][df['image_size']==image_size][\"perf\"]) for image_size in df.columns]\n",
    "    print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
