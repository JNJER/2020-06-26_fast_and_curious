{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! I am  [Jean-Nicolas Jérémie](https://github.com/JNJER) and the goal of this benchmark is to offer a comparison between differents pre-trained image recognition's networks based on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for $1000$ labels. These different networks tested here are taken from the `torchvision.models` library : `AlexNet`, `VGG16`, `MobileNetV2` and `ResNet101`.\n",
    "\n",
    "Our use case is to measure the performance of a system which receives a sequence of images and has to make a decision as soon as possible, hence with `batch_size=1`. Specifically, we wish also to compare different computing architectures such as CPUs, desktop GPUs or other more exotic platform such as the Jetson TX2 (experiment 1). Additionally, we will implement some image transformations as up/down-sampling (experiment 2) or transforming to grayscale (experiment 3) to quantify their influence on the accuracy and computation time of each network.\n",
    "\n",
    "In this notebook, I will use the [Pytorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook was done during a master 1 internship at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2020-06-26_fast_and_curious).\n",
    "\n",
    "<!-- TEASER_END -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the benchmark\n",
    "\n",
    "Our coding strategy is to build up a small libray as a package of scripts in the `DCNN_benchmark` folder and to run all calls to that library from this notebook. This organization will be useful to run on specific hardware such as the Jetson card on the one hand and to visualize results in the notebook, on the other hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%mkdir -p DCNN_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries; definition of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile DCNN_benchmark/init.py\n",
    "\n",
    "# Importing libraries\n",
    "import os\n",
    "import time \n",
    "from time import strftime,gmtime\n",
    "import json\n",
    "import time \n",
    "import numpy as np\n",
    "import imageio\n",
    "from numpy import random\n",
    "from torchvision.datasets import ImageFolder\n",
    "# to plot\n",
    "import matplotlib.pyplot as plt\n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "# figure's variables\n",
    "fig_width = 20\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio\n",
    "phi = phi**2\n",
    "colors = ['b', 'r', 'k','g']\n",
    "\n",
    "# host & date's variables \n",
    "# HOST = os.uname()[1]\n",
    "HOST = 'jnjer-HP-Pavilion-Notebook'\n",
    "#datetag = strftime(\"%Y-%m-%d\", gmtime()) \n",
    "datetag = '2020-08-27'\n",
    "\n",
    "#dataset configuration\n",
    "\n",
    "image_size = 256 # default image resolution\n",
    "image_sizes = 2**np.arange(6, 10) # resolutions explored in experiment 2\n",
    "\n",
    "N_images_per_class = 10\n",
    "#i_labels = random.randint(1000, size=(N_labels)) # Random choice\n",
    "i_labels = [409, 530, 892, 487, 920, 704, 879, 963, 646, 620 ] # Pre-selected classes\n",
    "N_labels = len(i_labels)\n",
    "\n",
    "id_dl = ''\n",
    "root = 'data'\n",
    "folder = 'imagenet_classes_100'\n",
    "path = os.path.join(root, folder) # data path\n",
    "\n",
    "with open('ImageNet-Datasets-Downloader/imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "labels[0].split(', ')\n",
    "labels = [label.split(', ')[1].lower().replace('_', ' ') for label in labels]\n",
    "\n",
    "class_loader = 'ImageNet-Datasets-Downloader/imagenet_class_info.json'\n",
    "with open(class_loader, 'r') as fp: # get all the classes on the data_downloader\n",
    "    name = json.load(fp)\n",
    "\n",
    "# a reverse look-up-table giving the index of a given label (within the whole set of imagenet labels)\n",
    "reverse_labels = {}\n",
    "for i_label, label in enumerate(labels):\n",
    "    reverse_labels[label] = i_label\n",
    "# a reverse look-up-table giving the index of a given i_label (within the sub-set of classes)\n",
    "reverse_i_labels = {}\n",
    "for i_label, label in enumerate(i_labels):\n",
    "    reverse_i_labels[label] = i_label\n",
    "\n",
    "\n",
    "def pprint(message):\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "\n",
    "pprint('List of Pre-selected classes')\n",
    "# choosing the selected classes for recognition\n",
    "for i_label in i_labels: \n",
    "    print('label', i_label, '=', labels[i_label])\n",
    "    for key in name:\n",
    "        if name[key]['class_name'] == labels[i_label]:\n",
    "            id_dl += key + ' '\n",
    "pprint('label IDs = ' + str(id_dl) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DCNN_benchmark/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOST == 'fortytwo':\n",
    "    do_local = False \n",
    "    python_exec = \"KMP_DUPLICATE_LIB_OK=TRUE python3\"\n",
    "else :\n",
    "    do_local =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download of example images from ImageNet :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an [ImageNet dataloader](https://github.com/laurentperrinet/ImageNet-datasets-downloader) to populate a dataset based on the pre-selected or randoms classes listed in the `DCNN_benchmark/init.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_benchmark/dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_benchmark.init import *\n",
    "\n",
    "# check if the folder exist\n",
    "if os.path.isdir(path):\n",
    "    list_dir = os.listdir(path)\n",
    "    print(\"The folder \" , folder, \" already exists, it includes: \", list_dir)\n",
    "    \n",
    "# no folder, creating one \n",
    "else :\n",
    "    print(f\"No existing path match for this folder, creating a folder at {path}\")\n",
    "    os.makedirs(path)\n",
    "\n",
    "# if the folder is empty, download the images using the ImageNet-Datasets-Downloader\n",
    "if len(list_dir) < N_labels : \n",
    "    print('This folder do not have anough classes, downloading some more') \n",
    "    cmd =f\"python3 ImageNet-Datasets-Downloader/downloader.py -data_root {root} -data_folder {folder} -images_per_class {N_images_per_class} -use_class_list True  -class_list {id_dl} -multiprocessing_workers 0\"\n",
    "    print('Command to run : ', cmd)\n",
    "    os.system(cmd) # running it\n",
    "    list_dir = os.listdir(path)\n",
    "    \n",
    "elif len(os.listdir(path)) == N_labels :\n",
    "    print(f'The folder already contains : {len(list_dir)} classes')\n",
    "          \n",
    "else : # if there are to many folders delete some\n",
    "    print('The folder have to many classes, deleting some')\n",
    "    for elem in os.listdir(path):\n",
    "        contenu = os.listdir(f'{path}/{elem}')\n",
    "        if len(os.listdir(path)) > N_labels :\n",
    "            for x in contenu:\n",
    "                os.remove(f'{path}/{elem}/{x}') # delete exces folders\n",
    "            try:\n",
    "                os.rmdir(f'{path}/{elem}')\n",
    "            except:\n",
    "                os.remove(f'{path}/{elem}')\n",
    "    list_dir = os.listdir(path)\n",
    "    print(\"Now the folder \" , folder, f\" contains :\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !python3 {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained network's import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we worked on four differents pre-trained networks `Alexnet`, `Mobilenet`, `Resnet101` and `VGG16`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_benchmark/models.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_benchmark.init import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transform function for input's image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(image_size)),      # Resize the image to image_size x image_size pixels size.\n",
    "    transforms.CenterCrop(int(image_size-20)),  # Crop the image to (image_size-20) x (image_size-20) pixels around the center.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting\n",
    "    mean=[0.485, 0.456, 0.406],  #  its average and\n",
    "    std=[0.229, 0.224, 0.225]    #  its standard deviation at the specified values.              \n",
    "    )])\n",
    "\n",
    "\n",
    "image_dataset = ImageFolder(path, transform=transform) # save the dataset\n",
    "\n",
    "# imports networks with weights\n",
    "models = {} # get model's names\n",
    "\n",
    "models['alex'] = torchvision.models.alexnet(pretrained=True)\n",
    "models['vgg'] = torchvision.models.vgg16(pretrained=True)\n",
    "models['mob'] = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "models['res'] = torchvision.models.resnext101_32x8d(pretrained=True)\n",
    "\n",
    "\n",
    "# Select a device (CPU or CUDA)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name in models.keys():\n",
    "    models[name].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Image processing and recognition for differents labels :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the classification confidence of the models according to the classes on which they have been trained, i.e. the $1000$ classes of the `ImageNet` library, the `softmax` mathematical function is added at the last layer of the networks. The `softmax` function is a function which takes a vector of real values (here represented by a 1-D tensor) of dimension `K` (here `K=1000` trained classes) and returns for each of these values a normalized propability between $0$ and $1$ with a sum equal to $1$. Thus, all the classes are represented in the final vector and a low probability would then be a proof of absence for instance. A careful reading of the original [imagenet paper](http://image-net.org/papers/imagenet_cvpr09.pdf) shows that this probability reflects the response of users to questions such as \"Is there a *Burmese cat* in the images?\" when presented an image (retrieved on internet) which is likely to include \"Burmese cat\".\n",
    "\n",
    "Here, we are interested in a sub-set of such classes. Nevertheless, the recognition being carried out on so-called \"natural\" images of the irrelevant classes could \"mask\" the recognition of those of interest. To reduce this effect, we have applied a slight modification to the output `softmax` function, by assuming that we know *a priori* that the image belongs to one (and only one) category from the sub-set, but that we do not know which one. As a consequence, it does not recover a vector of $K = 1000$ but of $K = N_{labels}$. As a consequence, the probabilities obtained would correspond to a confidence of classification discriminating only the classes of interest and can be compared to a chance level of $1 / N_{labels}$. \n",
    "\n",
    "For further statistical analyses, we extract these differents factors (like the accuracy and the processing time for differents datasets at differents resolution) in a `pandas` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_basic.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(filename)\n",
    "except:\n",
    "    df = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "    \n",
    "    # image preprocessing\n",
    "    for i_image, (data, label) in enumerate(image_dataset):\n",
    "        for name in models.keys():\n",
    "            model = models[name]\n",
    "            model.eval()\n",
    "            tic = time.time()\n",
    "            out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "            percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "            _, indices = torch.sort(percentage, descending=True)           \n",
    "            dt = time.time() - tic\n",
    "            i_label_top = reverse_labels[image_dataset.classes[label]]\n",
    "            perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "            df.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                               'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                               'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'device':str(device)}\n",
    "            print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "            i_trial += 1\n",
    "    df.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents labels display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect our results, we can already display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_1_{HOST}.json'\n",
    "df = pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents computation time of each models on the same dataset for the sequence of trials :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    #axs.set_ylim(0, 1)\n",
    "    df[df['model']==name]['time'].plot(label=name, color=color, marker='s', lw=0)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Processed on : '  + HOST + '_' + str(df['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the frequency of the classification performance for our four models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models), 1, figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for ax, color, name in zip(axs, colors, models.keys()):\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    df[df['model']==name]['perf'].plot.hist(bins=np.linspace(0, 100, 100), lw=1, label=name,ax=ax, color=color, density=True)\n",
    "    ax.legend(loc='upper left', fontsize = 20)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "axs[-1].set_xlabel('Classification performance (%)', size= 18)\n",
    "axs[0].set_title('Processed on : ' + HOST + '_' + str(df['device'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the 64 *worsts* classification performance, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df.loc[idx]['label'] + ' | ' + df.loc[idx]['model'], color='g')\n",
    "    perf_ = df.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it even clearer we extracted a specific median for each models : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"perf\"])\n",
    "    print(f'For the {name} model, the median clasification performance =  {med_perf:.2f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time 's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"time\"])\n",
    "    print(f'For the {name} model, the median computation time  =  {med_perf:.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame per second's median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf = np.median(df[df['model']==name][\"fps\"])\n",
    "    print(f'For the {name} model, the median fps  =  {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the model which displays the best accuracy is the `Resnet_101` network. However, the cost for such a high accuracy is reflected in the computation time as the `Resnet_101` also presents the higher computation time to process an image. Note that the `Mobilenet` network shows a good accuracy (>95%), while keeping the computation time reasonable. This results into a higher frame rate (images processed per second) that allows near to real-time recognition on a standard camera such as a webcam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Image processing and recognition for differents resolutions :\n",
    "\n",
    "Let's now study that same performance indicators at different image resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_downsample.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "\n",
    "# Output's set up\n",
    "try:\n",
    "    df_downsample = pd.read_json(filename)\n",
    "except:\n",
    "    df_downsample = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'image_size', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "\n",
    "    # image preprocessing\n",
    "    for image_size in image_sizes:\n",
    "        image_size = int(image_size)\n",
    "        transform = transforms.Compose([  # Downsampling function on the input\n",
    "        transforms.Resize(image_size),      #  Resize the image to image_size x image_size pixels size.\n",
    "        transforms.CenterCrop(image_size),  # Crop the image to image_size x image_size pixels around the center.\n",
    "        transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "        transforms.Normalize(        # Normalize the image by adjusting its average and\n",
    "                                     # its standard deviation at the specified values.\n",
    "        mean=[0.485, 0.456, 0.406],                \n",
    "        std=[0.229, 0.224, 0.225]                  \n",
    "        )])\n",
    "        image_dataset_downsample = ImageFolder(path, transform=transform) # Get the downsample dataset\n",
    "        print(f'Résolution de {image_size}')\n",
    "        # Displays the input image of the model \n",
    "        for i_image, (data, label) in enumerate(image_dataset_downsample):\n",
    "            for name in models.keys():\n",
    "                model = models[name]\n",
    "                model.eval()\n",
    "                tic = time.time()\n",
    "                out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "                percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "                _, indices = torch.sort(percentage, descending=True)           \n",
    "                dt = time.time() - tic\n",
    "                i_label_top = reverse_labels[image_dataset_downsample.classes[label]]\n",
    "                perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "                df_downsample.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'image_size': image_size, 'device':str(device)}\n",
    "                print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "                i_trial += 1\n",
    "        df_downsample.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents resolutions display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, again, we collect our results, and display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "df_downsample = pd.read_json(filename)\n",
    "df_downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display of the accuracy of each models on the same dataset for differents resolutions. Here accuracies are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"perf\", data=df_downsample, inner=\"quartile\", hue='model', cut = 0)\n",
    "    axs.set_title('Processed on : ' + HOST + '_' + str(df_downsample['device'][0]), size=20)\n",
    "    axs.set_ylabel('Classification performance (%)', size=18)\n",
    "    axs.set_xlabel('Image size', size=18)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:4], l[:4], loc ='center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 64 worsts classification performance, all models and sizes combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df_downsample[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df_downsample.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_downsample.loc[idx]['label'] + ' | ' + df_downsample.loc[idx]['model']+ ' | ' + str(df_downsample.loc[idx]['image_size']), color='g')\n",
    "    perf_ = df_downsample.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents computation time of each models on the same dataset for differents resolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"time\", data=df_downsample, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Processed on : ' + HOST + '_' + str(df_downsample['device'][0]), size = 20)\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    axs.set_yscale('log')\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:4], l[:4], loc='upper center', fontsize=16);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we extracted a specific median for each models : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    pprint(f'Benchmarking model {name}')\n",
    "    for image_size in image_sizes:\n",
    "        med_perf = np.median(df_downsample[(df_downsample['model']==name) & (df_downsample['image_size']==image_size)][\"perf\"])\n",
    "        print(f'For size {image_size}, the median clasification performance =  {med_perf:.2f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification performance does not depend on the host (a priori :-) ) but the timing does (see almso the synthesis below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    pprint(f'Benchmarking model {name}')\n",
    "    for image_size in image_sizes:\n",
    "        med_perf = np.median(df_downsample[(df_downsample['model']==name) & (df_downsample['image_size']==image_size)][\"time\"])\n",
    "        print(f'For size {image_size}, the median  computation time =  {med_perf:.2f} s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    pprint(f'Benchmarking model {name}')\n",
    "    for image_size in image_sizes:\n",
    "        med_perf = np.median(df_downsample[(df_downsample['model']==name) & (df_downsample['image_size']==image_size)][\"fps\"])\n",
    "        print(f'For size {image_size}, the median fps  =  {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification performance reduces when the resolution is too low or, surprisingly, higher as the regular size of an input's image (which is trained usually with `128 x 128` pixels). Also, the computation time  seems proportional to the resolution, a higher resolution need a higher delay to compute the image on a CPU. A size of `128 x 128` pixels clearly stands out as an optimal compromise for these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Image processing and recognition on grayscale images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_grayscale.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_benchmark.models import *\n",
    "filename = f'results/{datetag}_results_3_{HOST}.json'\n",
    "\n",
    "# Output's set up\n",
    "try:\n",
    "    df_gray = pd.read_json(filename)\n",
    "except:\n",
    "    df_gray = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device']) \n",
    "    i_trial = 0\n",
    "    \n",
    "    # image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Grayscale(3),      # convert the image in grayscale\n",
    "    transforms.Resize(int(image_size)),      # Resize the image.\n",
    "    transforms.CenterCrop(int(image_size-20)), # Crop the image with a 20 pixels border.\n",
    "    transforms.ToTensor(),       # Convert the image to PyTorch Tensor data type.\n",
    "    transforms.Normalize(        # Normalize the image by adjusting its average and\n",
    "                                 #     its standard deviation at the specified values.\n",
    "    mean=[0.485, 0.456, 0.406],                \n",
    "    std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "    image_dataset_grayscale = ImageFolder(path, transform=transform) # Get the downsample dataset\n",
    "\n",
    "    # Displays the input image of the model\n",
    "    for i_image, (data, label) in enumerate(image_dataset_grayscale):\n",
    "            for name in models.keys():\n",
    "                model = models[name]\n",
    "                model.eval()\n",
    "                tic = time.time()\n",
    "                out = model(data.unsqueeze(0).to(device)).squeeze(0)\n",
    "                percentage = torch.nn.functional.softmax(out[i_labels], dim=0) * 100\n",
    "                _, indices = torch.sort(percentage, descending=True)           \n",
    "                dt = time.time() - tic\n",
    "                i_label_top = reverse_labels[image_dataset_grayscale.classes[label]]\n",
    "                perf_ = percentage[reverse_i_labels[i_label_top]].item()            \n",
    "                df_gray.loc[i_trial] = {'model':name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_dataset.imgs[i_image][0], 'device':str(device)}\n",
    "                print(f'The {name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds')\n",
    "                i_trial += 1\n",
    "    df_gray.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_local:\n",
    "    %run {scriptname}\n",
    "else:\n",
    "    !{python_exec} {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image recognition on differents labels with grayscale display :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all the results, displaying all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_3_{HOST}.json'\n",
    "df_gray = pd.read_json(filename)\n",
    "df_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the accuracy of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Frequency', size= 18)\n",
    "    axs.set_xlabel('Classification performance (%)', size= 18)\n",
    "    df_gray[df_gray['model']==name]['perf'].plot.hist(bins=np.linspace(0, 100, 20), lw=0, alpha=0.8, label=name)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Processed on : ' + HOST + '_' + str(df_gray['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 64 worsts classification performance, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 6\n",
    "N_image_j = 6\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(15, 15))\n",
    "for i, idx in enumerate(df_gray[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_dataset.imgs[df_gray.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_gray.loc[idx]['label'] + ' | ' + df_gray.loc[idx]['model'], color='g')\n",
    "    perf_ = df_gray.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'{perf_:2.1f}', color='g')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display of the differents computation time of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "for color, name in zip(colors, models.keys()):\n",
    "    axs.set_ylabel('Computation time  (s)', size= 18)\n",
    "    axs.set_xlabel('Trial', size= 18)\n",
    "    df_gray[df_gray['model']==name]['time'].plot(label=name, color=color, marker='s', lw=0)\n",
    "    axs.legend(loc=0, fontsize = 20)\n",
    "    axs.set_title('Processed on : ' + HOST + '_' + str(df_gray['device'][0]), size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the accuracy of each models on the same dataset for color versus grayscale images. Here accuracies are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(30, fig_width/phi))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "for color, df_, label in zip(['gray', 'red'], [df_gray, df], ['black', 'color']):\n",
    "    axs = sns.violinplot(x=\"model\", y=\"perf\", data=df_, inner=\"quartile\", cut=0, color=color, alpha=.5)\n",
    "    axs.set_title('Processed on : ' + HOST + '_' + str(df_['device'][0]), size=20)\n",
    "    axs.set_ylabel('Classification performance (%)', size=18)\n",
    "    axs.legend(['Grayscale', 'Regular'])\n",
    "    axs.set_xlabel('Model', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf_orig = np.median(df[df['model']==name][\"perf\"])\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"perf\"])\n",
    "    print(f'For the {name} model, the median clasification performance = {med_perf:.2f} % (color =  {med_perf_orig:.1f} % )' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf_orig = np.median(df[df['model']==name][\"time\"])\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"time\"])\n",
    "    print(f'For the {name} model, the median computation time  =  {med_perf:.3f} s  (color =  {med_perf_orig:.3f} s )' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    med_perf_orig = np.median(df[df['model']==name][\"fps\"])\n",
    "    med_perf = np.median(df_gray[df_gray['model']==name][\"fps\"])\n",
    "    print(f'For the {name} model, the median fps  =  {med_perf:.3f} Hz (color =  {med_perf_orig:.3f} Hz )' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grayscale transformation on the input seems to degrade the recognition accuracy for all the models as they perform on the same dataset. There is only a modest gain in processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final synthesis\n",
    "\n",
    "We have run the benchmark on various platforms, with or without GPU. Let's summarize the main message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTS = {'fortytwo':'iMac pro 36 cores', \n",
    "         'ai-int-desktop': 'NVIDIA Jetson TX2',\n",
    "         'jnjer-HP-Pavilion-Notebook' : 'Intel core i5 7th gen'\n",
    "        # 'inv-ope-de06': 'Dell station with GTX Tegra',\n",
    "        }\n",
    "for HOST in  HOSTS:\n",
    "    print('HOST:', HOST, ', device:', HOSTS[HOST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the classification performance is similar on different machines (as these algorithms are deterministic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,3]:\n",
    "    for HOST in HOSTS:\n",
    "        filename = f'results/{datetag}_results_{i}_{HOST}.json'\n",
    "        print(filename)\n",
    "        df = pd.read_json(filename)\n",
    "        for name in models.keys():\n",
    "            med_perf = np.median(df[df['model']==name][\"perf\"])\n",
    "            print(f'On host {HOSTS[HOST]}, for the {name} model, the median clasification performance =  {med_perf:.2f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that the computation time varies greatly depending on the platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,3]:\n",
    "    for HOST in HOSTS:\n",
    "        filename = f'results/{datetag}_results_{i}_{HOST}.json'\n",
    "        print(filename)\n",
    "        df = pd.read_json(filename)\n",
    "        for name in models.keys():\n",
    "            med_perf = np.median(df[df['model']==name][\"time\"])\n",
    "            print(f'On host {HOSTS[HOST]}, for the {name} model, the median computation time  =  {med_perf:.3f} s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experiment 2, we check on the different hosts the same trend of classification performance for different image size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HOST in HOSTS:\n",
    "    filename = f'results/{datetag}_results_2_{HOST}.json'\n",
    "    df = pd.read_json(filename)\n",
    "    for size in image_sizes :\n",
    "        print(f'Image size : {size}')\n",
    "        for name in models.keys():\n",
    "            med_perf = np.median(df[df['model']==name][df['image_size']==size][\"perf\"])\n",
    "            print(f'On host {HOSTS[HOST]}, for the {name} model, the median clasification performance =  {med_perf:.2f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that performance may differ due to the difference between the datasets automatically extracted on the differennt machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
